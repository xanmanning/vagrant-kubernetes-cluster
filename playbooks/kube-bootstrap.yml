---

- name: Bootstrap Kubernetes nodes
  become: true
  hosts: all
  vars:
    ansible_python_interpreter: /usr/bin/python3
    kube_prefix: "kube-"
    raid_level: 10
    raid_device: /dev/md127
    block_devices: []
    filesystem_type: ext4
    k3s_become_for_all: true
    k3s_release_version: v1.19
    k3s_install_hard_links: true
    k3s_etcd_datastore: true
    k3s_use_experimental: true
    interface_name: enp0s8
    ansible_interface: "ansible_{{ interface_name }}"
    k3s_server:
      disable:
        - servicelb
        - local-storage
      write-kubeconfig-mode: 644
      advertise-address: "{{ hostvars[inventory_hostname][ansible_interface].ipv4.address }}"
      bind-address: 0.0.0.0
      kube-controller-manager-arg:
        - "pod-eviction-timeout=2m"
        - "node-monitor-grace-period=30s"
    k3s_agent:
      flannel-iface: "{{ interface_name }}"
      node-ip: "{{ hostvars[inventory_hostname][ansible_interface].ipv4.address }}"
      node-external-ip: "{{ hostvars[inventory_hostname][ansible_interface].ipv4.address }}"
      node-name: "{{ inventory_hostname }}"
      node-label:
        - "NodeID={{ inventory_hostname }}"
      kubelet-arg:
        - "volume-plugin-dir=/var/lib/rancher/k3s/agent/kubelet/plugins_registry"
    k3s_server_manifests_templates:
      - "{{ playbook_dir }}/../templates/00-metallb.yaml"
      - "{{ playbook_dir }}/../templates/05-metallb-arp-config.yaml"
      - "{{ playbook_dir }}/../templates/10-longhorn-operator.yaml"

  pre_tasks:
    - name: Ensure chronyd is installed
      package:
        name: chrony
        state: present

    - name: Ensure chronyd in started
      service:
        name: chronyd
        state: started
        enabled: true

    - name: Ensure software RAID and iSCSI applications are installed
      package:
        name:
          - mdadm
          - lvm2
          - open-iscsi
          - libiscsi7
          - nfs-common
        state: present

    - name: Ensure all nodes are set to be control nodes
      set_fact:
        k3s_control_node: true

    - name: Ensure the first node is configured as the Primary master
      set_fact:
        k3s_primary_control_node: "{{ inventory_hostname }}"
      when: inventory_hostname == play_hosts[0]

    - name: Ensure the correct k3s_control_node_address is specified
      set_fact:
        k3s_control_node_address: "{{ hostvars[item][ansible_interface].ipv4.address }}"
      loop: "{{ play_hosts }}"
      when: hostvars[item].k3s_primary_control_node is defined
            and hostvars[item].k3s_primary_control_node

    - name: Check for RAID device
      stat:
        path: "{{ raid_device }}"
      register: check_raid_device

    - name: Provision RAID array
      block:
        - name: Ensure a list of block devices exists
          set_fact:
            block_devices: "{{ block_devices + [ '/dev/' + item.key ] }}"
          loop: "{{ lookup('dict', ansible_devices) }}"
          when: item.value.partitions | length < 1
                and item.key.find('loop') == -1
                and item.value.sectors | int > 204800

        - name: Ensure a string of block devices exists
          set_fact:
            block_devices_string: "{{ block_devices | join(' ') }}"

        - name: Ensure a count of block devices is created
          set_fact:
            block_device_count: "{{ block_devices | length }}"

        - name: Ensure a RAID array exists
          command: >
            /sbin/mdadm --create
              --verbose {{ raid_device }}
              --level={{ raid_level }}
              --raid-devices={{ block_device_count }} {{ block_devices_string }}
          args:
            creates: "{{ raid_device }}"

        - name: Ensure a partition exists on the RAID array
          parted:
            device: "{{ raid_device }}"
            label: gpt
            flags: [ lvm ]
            number: 1
            state: present

        - name: Ensure the RAID array is added to an LVM volume group
          lvg:
            vg: vg_pvstore
            pvs: "{{ raid_device }}p1"

        - name: Ensure a Logical Volume exists
          lvol:
            vg: vg_pvstore
            lv: lv_pvdata
            size: +80%FREE

        - name: Ensure a filesystem is on the logical volume
          filesystem:
            dev: /dev/mapper/vg_pvstore-lv_pvdata
            fstype: "{{ filesystem_type }}"
            resizefs: true

      when: not check_raid_device.stat.exists

    - name: Ensure mountpoint exists
      file:
        path: /var/lib/longhorn
        state: directory

    - name: Ensure volume is mounted
      mount:
        src: /dev/mapper/vg_pvstore-lv_pvdata
        path: /var/lib/longhorn
        fstype: "{{ filesystem_type }}"
        state: mounted

    - name: Ensure iscsid is started
      service:
        name: iscsid
        state: started
        enabled: true

  roles:
    - xanmanning.k3s

  tasks:

    - name: Download a copy of the admin config
      fetch:
        src: /etc/rancher/k3s/k3s.yaml
        dest: ../config
        flat: true
      when: k3s_control_node

    - name: Ensure config contains the control plane IP
      delegate_to: localhost
      run_once: true
      become: false
      lineinfile:
        path: ../config
        line: "    server: https://{{ k3s_control_node_address }}:6443"
        regexp: "^\\s+server: https:"

    - name: Wait for storage and networking to be ready
      delegate_to: "{{ play_hosts[0] }}"
      run_once: true
      shell: >
        set -o pipefail &&
          /usr/local/bin/kubectl get pods -o wide --all-namespaces | grep -vE "Running|Completed"
      args:
        executable: /bin/bash
      register: check_pods_ready
      until: check_pods_ready.stdout_lines | length < 2
      retries: 100
      delay: 10
      failed_when: check_pods_ready.rc > 0
      changed_when: false

    # This can be done AFTER all pods are ready, no need to wait.
    - name: Ensure traefik ingress controller is scaled up to the number of nodes
      delegate_to: "{{ play_hosts[0] }}"
      run_once: true
      shell: >
        set -o pipefail &&
          /usr/local/bin/kubectl scale --replicas={{ play_hosts | length }} -n kube-system deployment/traefik
      args:
        executable: /bin/bash
      register: ensure_traefik_scaled
      failed_when: ensure_traefik_scaled.rc > 0
      changed_when: '"scaled" in ensure_traefik_scaled.stdout'

    - name: Closing message
      delegate_to: localhost
      run_once: true
      debug:
        msg:
         - "+----------------------------------------------------------------+"
         - "|                          NOTICE                                |"
         - "+----------------------------------------------------------------+"
         - "| Kubernetes cluster is now running. You can administer the      |"
         - "| cluster by setting KUBECONFIG to point to the `config` file in |"
         - "| this directory. To do this, run the following:                 |"
         - "|                                                                |"
         - "|    export KUBECONFIG=config                                    |"
         - "|                                                                |"
         - "| Test the configuration with the following commands:            |"
         - "|                                                                |"
         - "|    kubectl cluster-info                                        |"
         - "|    kubectl get nodes                                           |"
         - "|                                                                |"
         - "| Enjoy!                                                         |"
         - "+----------------------------------------------------------------+"
